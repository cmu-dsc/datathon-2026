{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon 2026 - Humanitarian Funding Analysis\n",
    "## Team Submission: Crisis Funding Prediction & Effectiveness Scoring\n",
    "\n",
    "This notebook contains our complete pipeline for:\n",
    "1. **Data Loading** - Pre-merged dataset with 64 features\n",
    "2. **Effectiveness Scoring** - Evaluating crisis response quality (Outcome-First: 20/20/40/20)\n",
    "3. **Model Building** - Predicting optimal funding levels\n",
    "4. **Visualizations** - Presentation-ready charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Load Pre-Merged Dataset\n",
    "\n",
    "This dataset was created by merging:\n",
    "- **INFORM Severity** (56 monthly files, 2020-2025)\n",
    "- **CERF Allocations** (UN emergency fund)\n",
    "- **CBPF Budgets** (Country pooled funds)\n",
    "- **HRP Requirements** (Humanitarian Response Plans)\n",
    "- **FTS Funding** (Global requirements vs actual funding)\n",
    "- **World Bank** (GDP, Population, Inflation)\n",
    "- **OCHA Demographics** (IDPs, Refugees, Vulnerable populations)\n",
    "- **HPC Cluster Data** (Humanitarian sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-merged dataset with all 64 features\n",
    "df = pd.read_csv('complete_funding_dataset.csv')\n",
    "\n",
    "print(f\"Dataset: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"Countries: {df['ISO3'].nunique()}\")\n",
    "print(f\"Year range: {df['Year'].min():.0f} - {df['Year'].max():.0f}\")\n",
    "print(f\"\\nColumns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data overview\n",
    "print(\"\\n=== Funding Summary ===\")\n",
    "print(f\"FTS Requirements: ${df['FTS_Requirements'].sum():,.0f}\")\n",
    "print(f\"FTS Actual Funding: ${df['FTS_Funding'].sum():,.0f}\")\n",
    "print(f\"FTS Funding Gap: ${df['FTS_Funding_Gap'].sum():,.0f}\")\n",
    "print(f\"Average % Funded: {df['FTS_Percent_Funded'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\n=== Severity Summary ===\")\n",
    "print(f\"Mean INFORM Score: {df['INFORM_Mean'].mean():.2f}\")\n",
    "print(f\"High Severity Crises (>=4.0): {(df['INFORM_Mean'] >= 4.0).sum()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Effectiveness Scoring\n",
    "\n",
    "Our scoring system uses **Outcome-First weights**:\n",
    "- **Coverage** (20%): % of funding requirements met\n",
    "- **Efficiency** (20%): Funding per person in need\n",
    "- **Outcome** (40%): INFORM severity improvement over time\n",
    "- **Gap** (20%): Funding gap severity (inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectiveness scores are already calculated in the dataset\n",
    "print(\"=== Effectiveness Score Distribution ===\")\n",
    "print(df['Effectiveness_Category'].value_counts())\n",
    "\n",
    "print(f\"\\nMean Effectiveness Score: {df['Effectiveness_Score'].mean():.1f}\")\n",
    "print(f\"Good Crises (score >= 45): {df['Is_Good_Crisis'].sum()} / {len(df)} ({100*df['Is_Good_Crisis'].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 best managed crises\n",
    "print(\"=== Top 10 Best Managed Crises ===\")\n",
    "top_10 = df[df['FTS_Funding'] > 0].nlargest(10, 'Effectiveness_Score')\n",
    "display_cols = ['Country', 'Year', 'INFORM_Mean', 'FTS_Percent_Funded', 'Effectiveness_Score', 'Effectiveness_Category']\n",
    "top_10[display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical underfunded crises\n",
    "print(\"=== Critical Underfunded Crises (High Severity) ===\")\n",
    "critical = df[(df['FTS_Funding'] > 0) & (df['INFORM_Mean'] >= 3.5)].nsmallest(10, 'Effectiveness_Score')\n",
    "critical_cols = ['Country', 'Year', 'INFORM_Mean', 'FTS_Percent_Funded', 'FTS_Funding_Gap', 'Effectiveness_Score']\n",
    "critical[critical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Model Building\n",
    "\n",
    "Train machine learning models to predict optimal funding levels based on crisis characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = [\n",
    "    # INFORM severity metrics\n",
    "    'INFORM_Mean', 'INFORM_Std', 'INFORM_Min', 'INFORM_Max',\n",
    "    'People_In_Need_Avg', 'Complexity_Avg', 'Impact_Avg',\n",
    "    \n",
    "    # Economic indicators\n",
    "    'GDP_Per_Capita', 'Inflation_Rate',\n",
    "    \n",
    "    # Population metrics  \n",
    "    'Population', 'Vulnerable_Pop_Pct', 'IDP_Rate',\n",
    "    \n",
    "    # Crisis metrics\n",
    "    'Number_Clusters', 'Total_In_Need', 'Coverage_Rate',\n",
    "    \n",
    "    # Derived features\n",
    "    'Need_Per_Capita', 'Economic_Stress',\n",
    "]\n",
    "\n",
    "categorical_features = ['Crisis_Type', 'UN_Region']\n",
    "\n",
    "# Target variable\n",
    "TARGET = 'FTS_Funding'\n",
    "\n",
    "# Filter to rows with valid target\n",
    "df_model = df[(df[TARGET].notna()) & (df[TARGET] > 0)].copy()\n",
    "print(f\"Training samples: {len(df_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "available_numeric = [f for f in numeric_features if f in df_model.columns]\n",
    "print(f\"Available numeric features: {len(available_numeric)}\")\n",
    "\n",
    "# Fill missing values with median\n",
    "X_numeric = df_model[available_numeric].copy()\n",
    "for col in available_numeric:\n",
    "    X_numeric[col] = X_numeric[col].fillna(X_numeric[col].median())\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_categorical = pd.DataFrame()\n",
    "for cat_col in categorical_features:\n",
    "    if cat_col in df_model.columns:\n",
    "        dummies = pd.get_dummies(df_model[cat_col], prefix=cat_col, drop_first=True)\n",
    "        X_categorical = pd.concat([X_categorical, dummies], axis=1)\n",
    "        print(f\"  {cat_col}: {df_model[cat_col].nunique()} categories\")\n",
    "\n",
    "# Combine features\n",
    "X = pd.concat([X_numeric.reset_index(drop=True), X_categorical.reset_index(drop=True)], axis=1)\n",
    "y = df_model[TARGET].reset_index(drop=True)\n",
    "y_log = np.log1p(y)  # Log transform for better distribution\n",
    "\n",
    "print(f\"\\nFeature matrix: {X.shape}\")\n",
    "print(f\"Target range: ${y.min():,.0f} to ${y.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=10, min_samples_split=5, \n",
    "    min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_log = rf.predict(X_test)\n",
    "y_pred_rf = np.expm1(y_pred_rf_log)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "# Metrics\n",
    "rf_r2 = r2_score(y_test, y_pred_rf_log)\n",
    "rf_mae = mean_absolute_error(y_test_actual, y_pred_rf)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_rf))\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  R² Score: {rf_r2:.4f}\")\n",
    "print(f\"  MAE: ${rf_mae:,.0f}\")\n",
    "print(f\"  RMSE: ${rf_rmse:,.0f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y_log, cv=5, scoring='r2')\n",
    "print(f\"  CV R² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "    min_samples_split=5, random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_log = gb.predict(X_test)\n",
    "y_pred_gb = np.expm1(y_pred_gb_log)\n",
    "\n",
    "gb_r2 = r2_score(y_test, y_pred_gb_log)\n",
    "gb_mae = mean_absolute_error(y_test_actual, y_pred_gb)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_gb))\n",
    "\n",
    "print(f\"\\nGradient Boosting Results:\")\n",
    "print(f\"  R² Score: {gb_r2:.4f}\")\n",
    "print(f\"  MAE: ${gb_mae:,.0f}\")\n",
    "print(f\"  RMSE: ${gb_rmse:,.0f}\")\n",
    "\n",
    "cv_scores_gb = cross_val_score(gb, X, y_log, cv=5, scoring='r2')\n",
    "print(f\"  CV R² Score: {cv_scores_gb.mean():.4f} (+/- {cv_scores_gb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== Top 15 Features Predicting Funding ===\")\n",
    "print(feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Generate Predictions & Identify Funding Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset for prediction\n",
    "X_full_numeric = df[available_numeric].copy()\n",
    "for col in available_numeric:\n",
    "    X_full_numeric[col] = X_full_numeric[col].fillna(X_full_numeric[col].median())\n",
    "\n",
    "X_full_categorical = pd.DataFrame()\n",
    "for cat_col in categorical_features:\n",
    "    if cat_col in df.columns:\n",
    "        dummies = pd.get_dummies(df[cat_col], prefix=cat_col, drop_first=True)\n",
    "        X_full_categorical = pd.concat([X_full_categorical, dummies], axis=1)\n",
    "\n",
    "X_full = pd.concat([X_full_numeric.reset_index(drop=True), X_full_categorical.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Ensure columns match training data\n",
    "for col in X.columns:\n",
    "    if col not in X_full.columns:\n",
    "        X_full[col] = 0\n",
    "X_full = X_full[X.columns]\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_full_log = rf.predict(X_full)\n",
    "y_pred_full = np.expm1(y_pred_full_log)\n",
    "\n",
    "df['Predicted_Funding'] = y_pred_full\n",
    "df['Actual_Funding'] = df['FTS_Funding'].fillna(0)\n",
    "df['Model_Funding_Gap'] = df['Predicted_Funding'] - df['Actual_Funding']\n",
    "\n",
    "print(\"Predictions generated for all crises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize funding status based on model predictions\n",
    "def categorize_funding(row):\n",
    "    if row['Actual_Funding'] == 0:\n",
    "        return 'No Funding Data'\n",
    "    gap_pct = (row['Model_Funding_Gap'] / row['Predicted_Funding']) * 100 if row['Predicted_Funding'] > 0 else 0\n",
    "    if gap_pct > 50: return 'Severely Underfunded'\n",
    "    elif gap_pct > 20: return 'Underfunded'\n",
    "    elif gap_pct > -20: return 'Adequately Funded'\n",
    "    else: return 'Well Funded'\n",
    "\n",
    "df['Funding_Status'] = df.apply(categorize_funding, axis=1)\n",
    "\n",
    "print(\"\\n=== Funding Status Distribution ===\")\n",
    "print(df['Funding_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top underfunded crises\n",
    "print(\"\\n=== Top 10 Underfunded High-Severity Crises ===\")\n",
    "underfunded = df[\n",
    "    (df['Actual_Funding'] > 0) & \n",
    "    (df['INFORM_Mean'] >= 3.0) &\n",
    "    (df['Model_Funding_Gap'] > 0)\n",
    "].nlargest(10, 'Model_Funding_Gap')\n",
    "\n",
    "display_cols = ['Country', 'Year', 'INFORM_Mean', 'Actual_Funding', 'Predicted_Funding', 'Model_Funding_Gap', 'Funding_Status']\n",
    "underfunded[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "models = ['Random Forest', 'Gradient Boosting']\n",
    "r2_scores = [rf_r2, gb_r2]\n",
    "mae_scores = [rf_mae/1e6, gb_mae/1e6]\n",
    "rmse_scores = [rf_rmse/1e6, gb_rmse/1e6]\n",
    "\n",
    "colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "bars1 = axes[0].bar(models, r2_scores, color=colors, edgecolor='white', linewidth=2)\n",
    "axes[0].set_ylabel('R² Score', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy (R²)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for bar, val in zip(bars1, r2_scores):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "bars2 = axes[1].bar(models, mae_scores, color=colors, edgecolor='white', linewidth=2)\n",
    "axes[1].set_ylabel('MAE (Millions USD)', fontsize=12)\n",
    "axes[1].set_title('Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars2, mae_scores):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                 f'${val:.0f}M', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "bars3 = axes[2].bar(models, rmse_scores, color=colors, edgecolor='white', linewidth=2)\n",
    "axes[2].set_ylabel('RMSE (Millions USD)', fontsize=12)\n",
    "axes[2].set_title('Root Mean Squared Error', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars3, rmse_scores):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                 f'${val:.0f}M', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_features = feature_importance.head(12)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_features)))\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'], fontsize=11)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_title('Top 12 Features Predicting Funding Needs', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, top_features['Importance']):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.1%}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Funding Status Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart\n",
    "status_counts = df['Funding_Status'].value_counts()\n",
    "colors_status = {'Well Funded': '#27ae60', 'Adequately Funded': '#3498db', \n",
    "                 'Underfunded': '#f39c12', 'Severely Underfunded': '#e74c3c',\n",
    "                 'No Funding Data': '#95a5a6'}\n",
    "pie_colors = [colors_status.get(s, '#95a5a6') for s in status_counts.index]\n",
    "\n",
    "wedges, texts, autotexts = axes[0].pie(status_counts.values, labels=status_counts.index, \n",
    "                                        autopct='%1.1f%%', colors=pie_colors, startangle=90,\n",
    "                                        explode=[0.05 if 'Under' in s else 0 for s in status_counts.index])\n",
    "axes[0].set_title('Crisis Funding Status Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Effectiveness score histogram\n",
    "axes[1].hist(df['Effectiveness_Score'].dropna(), bins=25, color='#3498db', edgecolor='white', alpha=0.8)\n",
    "axes[1].axvline(x=45, color='#e74c3c', linestyle='--', linewidth=2, label='Good Crisis Threshold (45)')\n",
    "axes[1].axvline(x=df['Effectiveness_Score'].mean(), color='#27ae60', linestyle='-', linewidth=2, \n",
    "                label=f'Mean ({df[\"Effectiveness_Score\"].mean():.1f})')\n",
    "axes[1].set_xlabel('Effectiveness Score', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Effectiveness Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predictions vs Actual\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_data = df[df['Actual_Funding'] > 0].copy()\n",
    "\n",
    "status_colors = {'Well Funded': '#27ae60', 'Adequately Funded': '#3498db', \n",
    "                 'Underfunded': '#f39c12', 'Severely Underfunded': '#e74c3c'}\n",
    "\n",
    "for status, color in status_colors.items():\n",
    "    mask = plot_data['Funding_Status'] == status\n",
    "    ax.scatter(plot_data.loc[mask, 'Actual_Funding']/1e9, \n",
    "               plot_data.loc[mask, 'Predicted_Funding']/1e9,\n",
    "               c=color, label=status, alpha=0.7, s=60, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "max_val = max(plot_data['Actual_Funding'].max(), plot_data['Predicted_Funding'].max()) / 1e9\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='Perfect Prediction', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Actual Funding (Billions USD)', fontsize=12)\n",
    "ax.set_ylabel('Predicted Funding (Billions USD)', fontsize=12)\n",
    "ax.set_title('Model Predictions vs Actual Funding', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Funding Trends by Year\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "yearly = df.groupby('Year').agg({\n",
    "    'FTS_Funding': 'sum',\n",
    "    'FTS_Requirements': 'sum',\n",
    "    'FTS_Funding_Gap': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "x = yearly.index.astype(int)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, yearly['FTS_Requirements']/1e9, width, label='Requirements', color='#3498db', edgecolor='white')\n",
    "ax.bar(x + width/2, yearly['FTS_Funding']/1e9, width, label='Actual Funding', color='#27ae60', edgecolor='white')\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Amount (Billions USD)', fontsize=12)\n",
    "ax.set_title('Humanitarian Funding: Requirements vs Reality', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "# Add % funded annotation\n",
    "for i, (req, fund) in enumerate(zip(yearly['FTS_Requirements']/1e9, yearly['FTS_Funding']/1e9)):\n",
    "    pct = (fund/req)*100 if req > 0 else 0\n",
    "    ax.annotate(f'{pct:.0f}%', xy=(x[i], fund), xytext=(x[i], fund + 2),\n",
    "                ha='center', fontsize=9, color='#27ae60', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Top Underfunded Crises\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "underfunded_viz = df[\n",
    "    (df['Actual_Funding'] > 0) & \n",
    "    (df['INFORM_Mean'] >= 3.0) &\n",
    "    (df['Model_Funding_Gap'] > 0)\n",
    "].nlargest(15, 'Model_Funding_Gap').copy()\n",
    "\n",
    "underfunded_viz['Label'] = underfunded_viz['Country'] + ' (' + underfunded_viz['Year'].astype(int).astype(str) + ')'\n",
    "underfunded_viz = underfunded_viz.sort_values('Model_Funding_Gap')\n",
    "\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(underfunded_viz)))\n",
    "bars = ax.barh(range(len(underfunded_viz)), underfunded_viz['Model_Funding_Gap']/1e6, color=colors)\n",
    "ax.set_yticks(range(len(underfunded_viz)))\n",
    "ax.set_yticklabels(underfunded_viz['Label'], fontsize=11)\n",
    "ax.set_xlabel('Funding Gap (Millions USD)', fontsize=12)\n",
    "ax.set_title('Top 15 Underfunded High-Severity Crises', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, underfunded_viz['Model_Funding_Gap']/1e6):\n",
    "    ax.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2, \n",
    "            f'${val:.0f}M', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary & Key Findings\n",
    "\n",
    "## Model Performance\n",
    "- **Best Model**: Gradient Boosting with R² ≈ 0.74\n",
    "- **Key Predictors**: INFORM severity (28%), People in Need (17%), INFORM Max (15%)\n",
    "\n",
    "## Effectiveness Scoring (Outcome-First: 20/20/40/20)\n",
    "- **Coverage**: 20% weight - % of requirements funded\n",
    "- **Efficiency**: 20% weight - $ per person in need\n",
    "- **Outcome**: 40% weight - INFORM severity improvement\n",
    "- **Gap**: 20% weight - Funding gap severity\n",
    "\n",
    "## Key Insights\n",
    "1. **$96 billion funding gap** over 2020-2025\n",
    "2. **71% average funding coverage** - crises receive about 71% of requested\n",
    "3. **Top underfunded**: Afghanistan, Yemen, Mali, DRC, Haiti\n",
    "4. **Model identifies funding gaps** where actual < predicted \"optimal\"\n",
    "\n",
    "## Recommendations\n",
    "1. Prioritize severely underfunded high-severity crises\n",
    "2. Use model predictions to guide resource allocation\n",
    "3. Focus on outcome improvement, not just funding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {len(df)} country-year records, {df['ISO3'].nunique()} countries\")\n",
    "print(f\"Year range: {df['Year'].min():.0f} - {df['Year'].max():.0f}\")\n",
    "\n",
    "print(f\"\\n--- Funding ---\")\n",
    "print(f\"Total Requirements: ${df['FTS_Requirements'].sum():,.0f}\")\n",
    "print(f\"Total Funding: ${df['FTS_Funding'].sum():,.0f}\")\n",
    "print(f\"Total Gap: ${df['FTS_Funding_Gap'].sum():,.0f}\")\n",
    "print(f\"Average % Funded: {df['FTS_Percent_Funded'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\n--- Model Performance ---\")\n",
    "print(f\"Random Forest R²: {rf_r2:.4f}\")\n",
    "print(f\"Gradient Boosting R²: {gb_r2:.4f}\")\n",
    "print(f\"Best Model: {'Gradient Boosting' if gb_r2 > rf_r2 else 'Random Forest'}\")\n",
    "\n",
    "print(f\"\\n--- Effectiveness Scoring ---\")\n",
    "print(f\"Good Crises (score >= 45): {df['Is_Good_Crisis'].sum()} ({100*df['Is_Good_Crisis'].mean():.1f}%)\")\n",
    "print(f\"Mean Effectiveness Score: {df['Effectiveness_Score'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\n--- Funding Status ---\")\n",
    "for status in ['Severely Underfunded', 'Underfunded', 'Adequately Funded', 'Well Funded']:\n",
    "    count = (df['Funding_Status'] == status).sum()\n",
    "    print(f\"  {status}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "output_cols = ['ISO3', 'Year', 'Country', 'INFORM_Mean', 'Crisis_Type', 'UN_Region',\n",
    "               'FTS_Funding', 'Predicted_Funding', 'Model_Funding_Gap', 'Funding_Status',\n",
    "               'Effectiveness_Score', 'Effectiveness_Category', 'Is_Good_Crisis']\n",
    "df[output_cols].to_csv('final_predictions.csv', index=False)\n",
    "print(\"Saved: final_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
