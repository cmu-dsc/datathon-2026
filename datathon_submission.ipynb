{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon 2026 - Humanitarian Funding Analysis\n",
    "## Team Submission: Crisis Funding Prediction & Effectiveness Scoring\n",
    "\n",
    "This notebook contains our complete pipeline for:\n",
    "1. **Data Preparation** - Merging INFORM severity with financial data\n",
    "2. **Feature Engineering** - Creating predictor variables\n",
    "3. **Effectiveness Scoring** - Evaluating crisis response quality\n",
    "4. **Model Building** - Predicting optimal funding levels\n",
    "5. **Visualizations** - Presentation-ready charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Loading & Preparation\n",
    "\n",
    "We start with the pre-processed INFORM severity data and merge it with financial data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INFORM severity data (pre-combined from 56 monthly Excel files)\n",
    "inform = pd.read_csv('inform_severity_combined.csv')\n",
    "print(f\"INFORM Severity Data: {len(inform)} rows, {len(inform.columns)} columns\")\n",
    "print(f\"Date range: {inform['month'].min()}/{inform['year'].min()} to {inform['month'].max()}/{inform['year'].max()}\")\n",
    "print(f\"Countries: {inform['ISO3'].nunique()}\")\n",
    "inform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate INFORM by Country-Year\n",
    "Convert monthly data to annual summaries with statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate INFORM data by country-year\n",
    "inform['year_num'] = inform['year'].astype(int)\n",
    "\n",
    "inform_agg = inform.groupby(['ISO3', 'year_num']).agg({\n",
    "    'INFORM Severity Index': ['mean', 'std', 'min', 'max', 'first', 'last'],\n",
    "    'CRISIS ID': 'nunique',\n",
    "    'COUNTRY': 'first',\n",
    "    'Regions': 'first',\n",
    "    'People in need': 'mean',\n",
    "    'Complexity of the crisis': 'mean',\n",
    "    'Impact of the crisis': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "inform_agg.columns = ['ISO3', 'Year', 'INFORM_Mean', 'INFORM_Std', 'INFORM_Min', 'INFORM_Max',\n",
    "                      'INFORM_Start', 'INFORM_End', 'Crisis_Count', 'Country', 'Region',\n",
    "                      'People_In_Need_Avg', 'Complexity_Avg', 'Impact_Avg']\n",
    "\n",
    "# Calculate change metrics\n",
    "inform_agg['INFORM_Change'] = inform_agg['INFORM_End'] - inform_agg['INFORM_Start']\n",
    "inform_agg['INFORM_Range'] = inform_agg['INFORM_Max'] - inform_agg['INFORM_Min']\n",
    "\n",
    "print(f\"Aggregated to {len(inform_agg)} country-year records\")\n",
    "inform_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Financial Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CERF allocations\n",
    "try:\n",
    "    cerf = pd.read_csv('data/project_targeting/Data_ CERF Donor Contributions and Allocations - allocations.csv')\n",
    "    cerf_agg = cerf.groupby(['countryCode', 'year']).agg({\n",
    "        'totalAmountApproved': 'sum',\n",
    "        'id': 'count'\n",
    "    }).reset_index()\n",
    "    cerf_agg.columns = ['ISO3', 'Year', 'CERF_Allocation', 'CERF_Project_Count']\n",
    "    print(f\"CERF: {len(cerf_agg)} country-year records, ${cerf_agg['CERF_Allocation'].sum():,.0f} total\")\n",
    "except:\n",
    "    print(\"CERF data not found - will use zeros\")\n",
    "    cerf_agg = pd.DataFrame(columns=['ISO3', 'Year', 'CERF_Allocation', 'CERF_Project_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CBPF data\n",
    "try:\n",
    "    cbpf = pd.read_csv('data/project_targeting/Data_ Country Based Pooled Funds (CBPF) - Projects.csv')\n",
    "    # Extract ISO3 from PooledFundName if needed\n",
    "    cbpf_agg = cbpf.groupby(['PooledFundISO3', 'AllocationYear']).agg({\n",
    "        'Budget': 'sum',\n",
    "        'ProjectID': 'count'\n",
    "    }).reset_index()\n",
    "    cbpf_agg.columns = ['ISO3', 'Year', 'CBPF_Budget', 'CBPF_Project_Count']\n",
    "    print(f\"CBPF: {len(cbpf_agg)} country-year records, ${cbpf_agg['CBPF_Budget'].sum():,.0f} total\")\n",
    "except:\n",
    "    print(\"CBPF data not found - will use zeros\")\n",
    "    cbpf_agg = pd.DataFrame(columns=['ISO3', 'Year', 'CBPF_Budget', 'CBPF_Project_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HRP data\n",
    "try:\n",
    "    hrp = pd.read_csv('data/geo_mismatch/humanitarian-response-plans.csv')\n",
    "    hrp['Year'] = pd.to_datetime(hrp['startDate']).dt.year\n",
    "    hrp_agg = hrp.groupby(['locations', 'Year']).agg({\n",
    "        'revisedRequirements': 'sum',\n",
    "        'id': 'count'\n",
    "    }).reset_index()\n",
    "    hrp_agg.columns = ['ISO3', 'Year', 'HRP_Revised_Requirements', 'HRP_Plan_Count']\n",
    "    print(f\"HRP: {len(hrp_agg)} country-year records, ${hrp_agg['HRP_Revised_Requirements'].sum():,.0f} total\")\n",
    "except:\n",
    "    print(\"HRP data not found - will use zeros\")\n",
    "    hrp_agg = pd.DataFrame(columns=['ISO3', 'Year', 'HRP_Revised_Requirements', 'HRP_Plan_Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FTS global funding data\n",
    "try:\n",
    "    fts = pd.read_csv('data/fts_requirements_funding_global.csv')\n",
    "    \n",
    "    # Skip HXL row if present\n",
    "    if fts.iloc[0]['countryCode'] == '#country+code':\n",
    "        fts = fts.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Convert types\n",
    "    fts['year'] = pd.to_numeric(fts['year'], errors='coerce')\n",
    "    fts['requirements'] = pd.to_numeric(fts['requirements'], errors='coerce')\n",
    "    fts['funding'] = pd.to_numeric(fts['funding'], errors='coerce')\n",
    "    \n",
    "    # Filter to 2020-2025\n",
    "    fts = fts[(fts['year'] >= 2020) & (fts['year'] <= 2025)]\n",
    "    \n",
    "    # Aggregate by country-year\n",
    "    fts_agg = fts.groupby(['countryCode', 'year']).agg({\n",
    "        'requirements': 'sum',\n",
    "        'funding': 'sum',\n",
    "        'name': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fts_agg['percentFunded'] = (fts_agg['funding'] / fts_agg['requirements'].replace(0, np.nan) * 100).fillna(0)\n",
    "    fts_agg['funding_gap'] = fts_agg['requirements'] - fts_agg['funding']\n",
    "    \n",
    "    fts_agg.columns = ['ISO3', 'Year', 'FTS_Requirements', 'FTS_Funding', 'FTS_Plan_Count', 'FTS_Percent_Funded', 'FTS_Funding_Gap']\n",
    "    print(f\"FTS: {len(fts_agg)} country-year records\")\n",
    "    print(f\"  Requirements: ${fts_agg['FTS_Requirements'].sum():,.0f}\")\n",
    "    print(f\"  Actual Funding: ${fts_agg['FTS_Funding'].sum():,.0f}\")\n",
    "    print(f\"  Funding Gap: ${fts_agg['FTS_Funding_Gap'].sum():,.0f}\")\n",
    "except Exception as e:\n",
    "    print(f\"FTS data not found: {e}\")\n",
    "    fts_agg = pd.DataFrame(columns=['ISO3', 'Year', 'FTS_Requirements', 'FTS_Funding', 'FTS_Plan_Count', 'FTS_Percent_Funded', 'FTS_Funding_Gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with INFORM aggregated data\n",
    "merged = inform_agg.copy()\n",
    "\n",
    "# Merge financial data\n",
    "merged = merged.merge(cerf_agg, on=['ISO3', 'Year'], how='left')\n",
    "merged = merged.merge(cbpf_agg, on=['ISO3', 'Year'], how='left')\n",
    "merged = merged.merge(hrp_agg, on=['ISO3', 'Year'], how='left')\n",
    "merged = merged.merge(fts_agg, on=['ISO3', 'Year'], how='left')\n",
    "\n",
    "# Fill missing financial data with 0\n",
    "financial_cols = ['CERF_Allocation', 'CBPF_Budget', 'HRP_Revised_Requirements', \n",
    "                  'FTS_Requirements', 'FTS_Funding', 'FTS_Funding_Gap']\n",
    "for col in financial_cols:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].fillna(0)\n",
    "\n",
    "# Calculate total funding\n",
    "merged['Total_Funding'] = merged[['CERF_Allocation', 'CBPF_Budget', 'HRP_Revised_Requirements']].sum(axis=1)\n",
    "\n",
    "print(f\"Merged Dataset: {len(merged)} rows, {len(merged.columns)} columns\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Feature Engineering\n",
    "\n",
    "Create derived features for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population data if available\n",
    "try:\n",
    "    pop = pd.read_csv('data/geo_mismatch/cod_population_admin0.csv')\n",
    "    pop_agg = pop.groupby(['ISO3', 'Reference_year']).agg({'Population': 'sum'}).reset_index()\n",
    "    pop_agg.columns = ['ISO3', 'Year', 'Population']\n",
    "    merged = merged.merge(pop_agg, on=['ISO3', 'Year'], how='left')\n",
    "    print(f\"Added population data\")\n",
    "except:\n",
    "    merged['Population'] = 1000000  # Default\n",
    "    print(\"Population data not found - using default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "merged['Population'] = merged['Population'].fillna(1000000)\n",
    "\n",
    "# Need per capita\n",
    "merged['Need_Per_Capita'] = (merged['People_In_Need_Avg'] / merged['Population'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# Funding per person in need\n",
    "merged['Funding_Per_PIN'] = (merged['FTS_Funding'] / merged['People_In_Need_Avg'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# Gap percentage\n",
    "merged['Gap_Percentage'] = ((merged['FTS_Funding_Gap'] / merged['FTS_Requirements'].replace(0, np.nan)) * 100).fillna(0).clip(0, 100)\n",
    "\n",
    "print(\"Derived features created\")\n",
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add crisis type categorization\n",
    "def categorize_crisis(row):\n",
    "    complexity = row.get('Complexity_Avg', 0) or 0\n",
    "    impact = row.get('Impact_Avg', 0) or 0\n",
    "    \n",
    "    if complexity > 3.5 and impact > 3.5:\n",
    "        return 'Complex'\n",
    "    elif complexity > 3:\n",
    "        return 'Conflict'\n",
    "    elif impact > 3:\n",
    "        return 'Natural Disaster'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "merged['Crisis_Type'] = merged.apply(categorize_crisis, axis=1)\n",
    "print(\"Crisis type distribution:\")\n",
    "print(merged['Crisis_Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UN Region classification\n",
    "region_map = {\n",
    "    'Eastern Africa': 'Sub-Saharan Africa', 'Western Africa': 'Sub-Saharan Africa',\n",
    "    'Middle Africa': 'Sub-Saharan Africa', 'Southern Africa': 'Sub-Saharan Africa',\n",
    "    'Northern Africa': 'MENA', 'Western Asia': 'MENA',\n",
    "    'Southern Asia': 'Asia', 'South-eastern Asia': 'Asia', 'Eastern Asia': 'Asia',\n",
    "    'Central Asia': 'Asia',\n",
    "    'Eastern Europe': 'Europe', 'Southern Europe': 'Europe', 'Western Europe': 'Europe',\n",
    "    'South America': 'LAC', 'Central America': 'LAC', 'Caribbean': 'LAC',\n",
    "}\n",
    "\n",
    "merged['UN_Region'] = merged['Region'].map(region_map).fillna('Other')\n",
    "print(\"UN Region distribution:\")\n",
    "print(merged['UN_Region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Effectiveness Scoring System\n",
    "\n",
    "Score crisis responses based on outcome improvement, funding coverage, and efficiency.\n",
    "\n",
    "**Weights (Outcome-First Approach):**\n",
    "- Coverage: 20%\n",
    "- Efficiency: 20%\n",
    "- Outcome: 40%\n",
    "- Gap: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize function\n",
    "def normalize_minmax(series, invert=False):\n",
    "    \"\"\"Normalize to 0-100 scale.\"\"\"\n",
    "    s = series.fillna(0)\n",
    "    min_val, max_val = s.min(), s.max()\n",
    "    if max_val == min_val:\n",
    "        return pd.Series([50] * len(s), index=s.index)\n",
    "    normalized = (s - min_val) / (max_val - min_val) * 100\n",
    "    return 100 - normalized if invert else normalized\n",
    "\n",
    "# Component scores (all on 0-100 scale)\n",
    "\n",
    "# Coverage Score: Higher FTS_Percent_Funded = better\n",
    "merged['Score_Coverage'] = merged['FTS_Percent_Funded'].clip(0, 100).fillna(0)\n",
    "\n",
    "# Efficiency Score: Higher funding per person in need = better (normalized)\n",
    "funding_per_pin_capped = merged['Funding_Per_PIN'].clip(0, merged['Funding_Per_PIN'].quantile(0.95))\n",
    "merged['Score_Efficiency'] = normalize_minmax(funding_per_pin_capped)\n",
    "\n",
    "# Outcome Score: Negative INFORM change = improvement = better\n",
    "merged['Score_Outcome'] = (50 - merged['INFORM_Change'].fillna(0) * 50).clip(0, 100)\n",
    "\n",
    "# Gap Score: Lower gap = better\n",
    "merged['Score_Gap'] = (100 - merged['Gap_Percentage'].fillna(50)).clip(0, 100)\n",
    "\n",
    "print(\"Component scores calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Effectiveness Score (Outcome-First weights)\n",
    "weights = {'coverage': 0.20, 'efficiency': 0.20, 'outcome': 0.40, 'gap': 0.20}\n",
    "\n",
    "merged['Effectiveness_Score'] = (\n",
    "    weights['coverage'] * merged['Score_Coverage'] +\n",
    "    weights['efficiency'] * merged['Score_Efficiency'] +\n",
    "    weights['outcome'] * merged['Score_Outcome'] +\n",
    "    weights['gap'] * merged['Score_Gap']\n",
    ")\n",
    "\n",
    "# Fallback for rows without FTS data\n",
    "no_fts_mask = merged['FTS_Funding'] == 0\n",
    "merged.loc[no_fts_mask, 'Effectiveness_Score'] = (\n",
    "    0.5 * merged.loc[no_fts_mask, 'Score_Outcome'] + 0.5 * 30\n",
    ")\n",
    "\n",
    "# Categorize\n",
    "def categorize_effectiveness(score):\n",
    "    if score >= 60: return 'Highly Effective'\n",
    "    elif score >= 45: return 'Moderately Effective'\n",
    "    elif score >= 30: return 'Needs Improvement'\n",
    "    else: return 'Critical - Underfunded'\n",
    "\n",
    "merged['Effectiveness_Category'] = merged['Effectiveness_Score'].apply(categorize_effectiveness)\n",
    "merged['Is_Good_Crisis'] = merged['Effectiveness_Score'] >= 45\n",
    "\n",
    "print(\"\\nEffectiveness Score Distribution:\")\n",
    "print(merged['Effectiveness_Category'].value_counts())\n",
    "print(f\"\\nGood Crises (score >= 45): {merged['Is_Good_Crisis'].sum()} / {len(merged)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Model Building\n",
    "\n",
    "Train machine learning models to predict optimal funding levels based on crisis characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = [\n",
    "    'INFORM_Mean', 'INFORM_Std', 'INFORM_Min', 'INFORM_Max',\n",
    "    'People_In_Need_Avg', 'Complexity_Avg', 'Impact_Avg',\n",
    "    'Population', 'Need_Per_Capita'\n",
    "]\n",
    "\n",
    "categorical_features = ['Crisis_Type', 'UN_Region']\n",
    "\n",
    "# Target variable\n",
    "TARGET = 'FTS_Funding'\n",
    "\n",
    "# Filter to rows with valid target\n",
    "df_model = merged[(merged[TARGET].notna()) & (merged[TARGET] > 0)].copy()\n",
    "print(f\"Training samples: {len(df_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "available_numeric = [f for f in numeric_features if f in df_model.columns]\n",
    "\n",
    "# Fill missing values\n",
    "X_numeric = df_model[available_numeric].copy()\n",
    "for col in available_numeric:\n",
    "    X_numeric[col] = X_numeric[col].fillna(X_numeric[col].median())\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_categorical = pd.DataFrame()\n",
    "for cat_col in categorical_features:\n",
    "    if cat_col in df_model.columns:\n",
    "        dummies = pd.get_dummies(df_model[cat_col], prefix=cat_col, drop_first=True)\n",
    "        X_categorical = pd.concat([X_categorical, dummies], axis=1)\n",
    "\n",
    "# Combine features\n",
    "X = pd.concat([X_numeric.reset_index(drop=True), X_categorical.reset_index(drop=True)], axis=1)\n",
    "y = df_model[TARGET].reset_index(drop=True)\n",
    "y_log = np.log1p(y)  # Log transform for better distribution\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Target range: ${y.min():,.0f} to ${y.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=10, min_samples_split=5, \n",
    "    min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf_log = rf.predict(X_test)\n",
    "y_pred_rf = np.expm1(y_pred_rf_log)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "# Metrics\n",
    "rf_r2 = r2_score(y_test, y_pred_rf_log)\n",
    "rf_mae = mean_absolute_error(y_test_actual, y_pred_rf)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_rf))\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  R² Score: {rf_r2:.4f}\")\n",
    "print(f\"  MAE: ${rf_mae:,.0f}\")\n",
    "print(f\"  RMSE: ${rf_rmse:,.0f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y_log, cv=5, scoring='r2')\n",
    "print(f\"  CV R² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "    min_samples_split=5, random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb_log = gb.predict(X_test)\n",
    "y_pred_gb = np.expm1(y_pred_gb_log)\n",
    "\n",
    "gb_r2 = r2_score(y_test, y_pred_gb_log)\n",
    "gb_mae = mean_absolute_error(y_test_actual, y_pred_gb)\n",
    "\n",
    "print(f\"\\nGradient Boosting Results:\")\n",
    "print(f\"  R² Score: {gb_r2:.4f}\")\n",
    "print(f\"  MAE: ${gb_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features Predicting Funding:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Generate Predictions & Identify Funding Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset for prediction\n",
    "X_full_numeric = merged[available_numeric].copy()\n",
    "for col in available_numeric:\n",
    "    X_full_numeric[col] = X_full_numeric[col].fillna(X_full_numeric[col].median())\n",
    "\n",
    "X_full_categorical = pd.DataFrame()\n",
    "for cat_col in categorical_features:\n",
    "    if cat_col in merged.columns:\n",
    "        dummies = pd.get_dummies(merged[cat_col], prefix=cat_col, drop_first=True)\n",
    "        X_full_categorical = pd.concat([X_full_categorical, dummies], axis=1)\n",
    "\n",
    "X_full = pd.concat([X_full_numeric.reset_index(drop=True), X_full_categorical.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Ensure columns match\n",
    "for col in X.columns:\n",
    "    if col not in X_full.columns:\n",
    "        X_full[col] = 0\n",
    "X_full = X_full[X.columns]\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_full_log = rf.predict(X_full)\n",
    "y_pred_full = np.expm1(y_pred_full_log)\n",
    "\n",
    "merged['Predicted_Funding'] = y_pred_full\n",
    "merged['Actual_Funding'] = merged['FTS_Funding'].fillna(0)\n",
    "merged['Funding_Gap'] = merged['Predicted_Funding'] - merged['Actual_Funding']\n",
    "\n",
    "print(\"Predictions generated for all crises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize funding status\n",
    "def categorize_funding(row):\n",
    "    if row['Actual_Funding'] == 0:\n",
    "        return 'No Funding Data'\n",
    "    gap_pct = (row['Funding_Gap'] / row['Predicted_Funding']) * 100 if row['Predicted_Funding'] > 0 else 0\n",
    "    if gap_pct > 50: return 'Severely Underfunded'\n",
    "    elif gap_pct > 20: return 'Underfunded'\n",
    "    elif gap_pct > -20: return 'Adequately Funded'\n",
    "    else: return 'Well Funded'\n",
    "\n",
    "merged['Funding_Status'] = merged.apply(categorize_funding, axis=1)\n",
    "\n",
    "print(\"\\nFunding Status Distribution:\")\n",
    "print(merged['Funding_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top underfunded crises\n",
    "print(\"\\nTop 10 Underfunded High-Severity Crises:\")\n",
    "underfunded = merged[\n",
    "    (merged['Actual_Funding'] > 0) & \n",
    "    (merged['INFORM_Mean'] >= 3.0) &\n",
    "    (merged['Funding_Gap'] > 0)\n",
    "].nlargest(10, 'Funding_Gap')\n",
    "\n",
    "display_cols = ['Country', 'Year', 'INFORM_Mean', 'Actual_Funding', 'Predicted_Funding', 'Funding_Gap']\n",
    "underfunded[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "models = ['Random Forest', 'Gradient Boosting']\n",
    "r2_scores = [rf_r2, gb_r2]\n",
    "mae_scores = [rf_mae/1e6, gb_mae/1e6]\n",
    "\n",
    "colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "axes[0].bar(models, r2_scores, color=colors)\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Model Accuracy (R²)', fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].bar(models, mae_scores, color=colors)\n",
    "axes[1].set_ylabel('MAE (Millions USD)')\n",
    "axes[1].set_title('Mean Absolute Error', fontweight='bold')\n",
    "\n",
    "# Feature importance\n",
    "top_features = feature_importance.head(10)\n",
    "axes[2].barh(range(len(top_features)), top_features['Importance'], color=plt.cm.viridis(np.linspace(0.2, 0.8, 10)))\n",
    "axes[2].set_yticks(range(len(top_features)))\n",
    "axes[2].set_yticklabels(top_features['Feature'])\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title('Top 10 Features', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Funding Status Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart\n",
    "status_counts = merged['Funding_Status'].value_counts()\n",
    "colors_status = {'Well Funded': '#27ae60', 'Adequately Funded': '#3498db', \n",
    "                 'Underfunded': '#f39c12', 'Severely Underfunded': '#e74c3c',\n",
    "                 'No Funding Data': '#95a5a6'}\n",
    "pie_colors = [colors_status.get(s, '#95a5a6') for s in status_counts.index]\n",
    "\n",
    "axes[0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', \n",
    "            colors=pie_colors, startangle=90)\n",
    "axes[0].set_title('Crisis Funding Status Distribution', fontweight='bold')\n",
    "\n",
    "# Effectiveness score histogram\n",
    "axes[1].hist(merged['Effectiveness_Score'].dropna(), bins=25, color='#3498db', edgecolor='white')\n",
    "axes[1].axvline(x=45, color='#e74c3c', linestyle='--', linewidth=2, label='Good Crisis Threshold (45)')\n",
    "axes[1].axvline(x=merged['Effectiveness_Score'].mean(), color='#27ae60', linestyle='-', linewidth=2, \n",
    "                label=f'Mean ({merged[\"Effectiveness_Score\"].mean():.1f})')\n",
    "axes[1].set_xlabel('Effectiveness Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Effectiveness Score Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predictions vs Actual\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_data = merged[merged['Actual_Funding'] > 0].copy()\n",
    "\n",
    "status_colors = {'Well Funded': '#27ae60', 'Adequately Funded': '#3498db', \n",
    "                 'Underfunded': '#f39c12', 'Severely Underfunded': '#e74c3c'}\n",
    "\n",
    "for status, color in status_colors.items():\n",
    "    mask = plot_data['Funding_Status'] == status\n",
    "    ax.scatter(plot_data.loc[mask, 'Actual_Funding']/1e9, \n",
    "               plot_data.loc[mask, 'Predicted_Funding']/1e9,\n",
    "               c=color, label=status, alpha=0.7, s=60)\n",
    "\n",
    "max_val = max(plot_data['Actual_Funding'].max(), plot_data['Predicted_Funding'].max()) / 1e9\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Actual Funding (Billions USD)')\n",
    "ax.set_ylabel('Predicted Funding (Billions USD)')\n",
    "ax.set_title('Model Predictions vs Actual Funding', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Funding Trends by Year\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "yearly = merged.groupby('Year').agg({\n",
    "    'FTS_Funding': 'sum',\n",
    "    'FTS_Requirements': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "x = yearly.index.astype(int)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, yearly['FTS_Requirements']/1e9, width, label='Requirements', color='#3498db')\n",
    "ax.bar(x + width/2, yearly['FTS_Funding']/1e9, width, label='Actual Funding', color='#27ae60')\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Amount (Billions USD)')\n",
    "ax.set_title('Humanitarian Funding: Requirements vs Reality', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary & Key Findings\n",
    "\n",
    "## Model Performance\n",
    "- **Best Model**: Gradient Boosting with R² = 0.74\n",
    "- **Key Predictors**: INFORM severity (28%), People in Need (17%), INFORM Max (15%)\n",
    "\n",
    "## Effectiveness Scoring (Outcome-First: 20/20/40/20)\n",
    "- **Coverage**: 20% weight - % of requirements funded\n",
    "- **Efficiency**: 20% weight - $ per person in need\n",
    "- **Outcome**: 40% weight - INFORM severity improvement\n",
    "- **Gap**: 20% weight - Funding gap severity\n",
    "\n",
    "## Key Insights\n",
    "1. **$96 billion funding gap** over 2020-2025\n",
    "2. **71% average funding coverage** - crises receive about 71% of requested\n",
    "3. **Top underfunded**: Afghanistan, Yemen, Mali, DRC, Haiti\n",
    "4. **Model identifies funding gaps** where actual < predicted \"optimal\"\n",
    "\n",
    "## Recommendations\n",
    "1. Prioritize severely underfunded high-severity crises\n",
    "2. Use model predictions to guide resource allocation\n",
    "3. Focus on outcome improvement, not just funding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final outputs\n",
    "merged.to_csv('final_submission_dataset.csv', index=False)\n",
    "print(f\"Saved final dataset: {len(merged)} rows, {len(merged.columns)} columns\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total crises analyzed: {len(merged)}\")\n",
    "print(f\"Countries: {merged['ISO3'].nunique()}\")\n",
    "print(f\"Date range: {merged['Year'].min():.0f} - {merged['Year'].max():.0f}\")\n",
    "print(f\"\\nFunding Gap: ${merged['FTS_Funding_Gap'].sum():,.0f}\")\n",
    "print(f\"Average % Funded: {merged['FTS_Percent_Funded'].mean():.1f}%\")\n",
    "print(f\"\\nGood Crises (Effectiveness >= 45): {merged['Is_Good_Crisis'].sum()}\")\n",
    "print(f\"Model R²: {max(rf_r2, gb_r2):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
